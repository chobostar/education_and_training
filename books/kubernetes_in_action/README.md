Kubernetes в действии. Марко Лукша
==================================

* [Kubernetes в действии. Марко Лукша](#kubernetes-в-действии-марко-лукша)
  * [Глава 1. Знакомство с K8S](#глава-1-знакомство-с-k8s)
    * [Запуск приложений в k8s](#запуск-приложений-в-k8s)
    * [Преимущества использования k8s](#преимущества-использования-k8s)
  * [Глава 2. Первые шаги с Docker и Kubernetes](#глава-2-первые-шаги-с-docker-и-kubernetes)
    * [docker commands](#docker-commands)
    * [k8s commands](#k8s-commands)
  * [Глава 3. Модули: запуск контейнеров в Kubernetes](#глава-3-модули-запуск-контейнеров-в-kubernetes)
    * [Создание модулей из дескрипторов YAML и JSON](#создание-модулей-из-дескрипторов-yaml-и-json)
      * [Label selectors](#label-selectors)
      * [Резюме](#резюме)
  * [Глава 4. Контроллер репликации и другие контроллеры: развертывание управляемых модулей](#глава-4-контроллер-репликации-и-другие-контроллеры-развертывание-управляемых-модулей)
    * [Резюме](#резюме-1)
  * [Глава 5. Службы: обеспечение клиентов возможностью обнаруживать модули и обмениваться с ними информацией](#глава-5-службы-обеспечение-клиентов-возможностью-обнаруживать-модули-и-обмениваться-с-ними-информацией)
    * [Предоставление внешний клиентам доступа к службам](#предоставление-внешний-клиентам-доступа-к-службам)
    * [Headless службы](#headless-службы)
    * [Устранение неполадок в службах](#устранение-неполадок-в-службах)
    * [Резюме](#резюме-2)
  * [Глава 6. Тома: подлкючение дискового хранилища к контейнерам](#глава-6-тома-подлкючение-дискового-хранилища-к-контейнерам)
    * [Динамическое резервирование томов PV](#динамическое-резервирование-томов-pv)
  * [Глава 7.Слвари конфигурации (ConfigMap) и секреты (Secret): настройка приложений](#глава-7слвари-конфигурации-configmap-и-секреты-secret-настройка-приложений)
    * [Обновление конфигурации приложения без перезапуска приложения](#обновление-конфигурации-приложения-без-перезапуска-приложения)
    * [Secrets](#secrets)
  * [Глава 8. Доступ к метаданным модуля и другим ресурсам из приложений](#глава-8-доступ-к-метаданным-модуля-и-другим-ресурсам-из-приложений)
  * [Взаимодействие с сервером API изнутри модуля](#взаимодействие-с-сервером-api-изнутри-модуля)
  * [Глава 9. Развертывания: декларативное обновление приложений](#глава-9-развертывания-декларативное-обновление-приложений)
    * [Управленение скоростью выкладки](#управленение-скоростью-выкладки)
  * [Глава 10. Ресурсы StatefulSet: развертывание реплицируемых приложений с внутренним состоянием](#глава-10-ресурсы-statefulset-развертывание-реплицируемых-приложений-с-внутренним-состоянием)
    * [Обнаружение соседей  в наборе StatefulSet](#обнаружение-соседей--в-наборе-statefulset)
    * [Как наборы StatefulSet справляются с аварийныеми сбоями узлов](#как-наборы-statefulset-справляются-с-аварийныеми-сбоями-узлов)

## Глава 1. Знакомство с K8S

Монолиты имеют медленные циклы релизов и редко обновляются. Монолиты дробятся на микросервисы, чтобы разрабатываться независимо и быстро. 
Нужна автоматика по деплойменту, настройке, контролю и обработке аварийных сбоев. Именно здесь в игру вступает Kubernetes.

Kubernetes:
- разработчики деплоят свои приложения самостоятельно и так часто
- разработчикам для деплоя не требуется помощь системных администраторов
- помогает автоматически отслеживать и перемещать приложения в случае сбоев оборудования
- сисадмины - контроль за платформой k8s и остальной инфраструктурой
- k8s сама заботится о приложениях

DevOps - разработка, QA и сисадмины  сотрудничают на протяжении всего процесса
NoOps - разработчики деплоят, не зная об аппаратной инфраструктуре и не имея дела с сисадминами

K8s позволяет достичь NoOps.

Приложения изолируют свои среды через контейнеры. Виртуалки требуется индивидуальной настройки и управления, 
требуют траты человеческих ресурсов.

Каждая VM требует запускать свой набор системных процессов, который требует еще вычислительных ресурсов в дополнение, 
что требуется приложением. Контейнер выполняется в центральной ОС, без накладных расходов в виде доп ресурсов.

namespaces - каждый процесс видит свое персональное представление о системе (файлы, процессы, сетевые интерфейсы, hostname итд)
cgroups - ограничивает объем ресурсов, которые может потреблять процесс (cpu, ram, network throthput etc)

Компоненты K8s:
- control plane (master):
    - etcd - непрерывно сохраняетт конфигурацию кластера
    - сервер API - то с чем взаимодействует другие компоненты и человек
    - менеджер контроллеров - выполняет функции кластерного уровня, как репликация компонентов, отслеж нод, обработка отказов итд
    - планировщик - распределяет приложения по нодам
- Рабочий узел (kube node):
    - kubelet - агент, общаяется с сервером API и управляет контейнерами на своем узле
    - docker runtime
    - kube-proxy - балалнсирует нагрузку сетевого трафика между компонентами приложения

#### Запуск приложений в k8s

Сборка приложения в один или несколько контейнеров, отправка их в хранилище образов, а затем публикация описания приложения на сервере API K8s

Когда сервер API обрабатывает описание приложения, планировщик назначает указанные группы контейнеров доступным рабочим узлам, исходя из
вычислительных ресурсов, требуемых каждой группой, и нераспределенных ресурсов на каждом узле в данных момент.
Агент Kubelet на этих узлах затем поручает среде выполнения контейнеров (docker runtime) извлечь из хранилища требуемые
образы контейнеров и запустить контейнеры.

После запуска приложения k8s следит за тем, чтобы то состояние, в котором было развернуто приложение, всегда соответствовало указанному вами описанию.

Можно сообщить k8s, какие контейнеры предосталяют одну и ту же службу, и k8s обеспечит доступ ко всем контейрнерам по единому статическому IP и предоставит
доступ к этому адресу всем приложениям, работающим в кластере. Это делается через env vars, но можно и через DNS. kube-proxy обеспечивает балансировку нагрузки
подключений к службе во всех контейнерах, предоставляющих службу. IP-адрес службы остается постоянным, поэтому клиенты всегда могут подключаться к ее контейнерам, даже
если они перемещаются по кластеру.

#### Преимущества использования k8s
- Упрощение развертывания приложения
- Повышение эффективности задействования оборудования
- Проверка здоровья и самолечение
- Автоматическое масштабироваие
- Упрощение разработки приложений

## Глава 2. Первые шаги с Docker и Kubernetes

#### docker commands
```
$ docker build -t kubia .
$ docker run --name kubia-container -p 8080:8080 -d kubia
$ curl localhost:8080
$ docker ps
$ docker inspect kubia-container
$ docker exec -it kubia-container bash
$ docker stop kubia-container
$ docker rm kubia-container
$ docker tag kubia somename/kubia
$ docker push somename/kubia
```

#### k8s commands
```
$ minikube start
$ kubectl cluster-info
$ kubectl get nodes
$ kubectl describe node minikube
$ kubectl run kubia --image=somename/kubia --port=8080 --generator=run/v1
$ kubectl get pods
$ kubectl expose rc kubia --type=LoadBalancer --name kubia-http
$ minikube serivce kubia-http
$ kubectl get services
$ kubectl get svc
$ kubectl get replicationcontrollers
$ kubectl scale rc kubia --replicas=3
$ kubectl get rc
$ kubectl get pods
$ kubectl get pods -o wide
$ kubectl describe pod <podname>
$ minikube dashboard
```

## Глава 3. Модули: запуск контейнеров в Kubernetes

Модули - размещенная рядом группа контейнеров, основной строительный блок в k8s:
- частичная изоляция между контейнерами одного модуля - один linux namespace: Network, UTS 
- использует одно пространство ip-адресов и портов
- один loopback-интерфейс
- все модули видят друг друга по ip - плоская сеть под модулями

модули являются логическими хостами и ведут себя как физические хосты или виртуальные машиы в неконтейнерном мире. Процессы
запущенные в одном модуле, подобны процессам, запущенным на одной физической или виртуальной машине, за исключением того, что
каждый процесс инкапсулируется в контейнер.

(здесь масштабирование - подразумевается горизонатальное масштабирование)

как контейнеры должны быть сгруппированы в модули:
- они должны работать вместе или могут работать на разных хостах?
- представляют ли они собой единое целое или являются независимымми компонентами?
- они должны масштабироваться вместе или по отдельности?

Контейнер не должен выполнять несколько процессов. 
Модуль не должен содержать многочисленные контейнеры, если они не обязатятельно должны выполняться на одинаковой машине.

### Создание модулей из дескрипторов YAML и JSON

Модули и другие ресурсы K8s обычно создаются путеем публикации манифеста json или yaml в конечной точке k8s rest api. Более простые - kubectl run.

```
kubectl get po kubia-zxzij -o yaml
```

Почти во всех ресурсах k89s находятся три каждые секции:
- metadata - вклчюает имя, пространство имен, метки и другую информацию о модуле;
- spec - содержит фактическое описание содержимого модуля, напрмер контейнеры модуля, тома и другие данные;
- status - содержит текущую информацию о работающем модуле, как условие, в котором находится модуль описание и статус каждого контейнера, внутренний IP модуля, и другую базовую информацию.

```
kubectl explain pods
```
```
kubectl create -f kubia-manual.yaml
```

#### Label selectors

- содержит ли (или не содержит) ресурс метку с определенным ключом;
- содержит ли ресурс метку с определенным ключом и значением;
- содержил ли ресурс метку с определенным ключом, но со значением, не равным указанному вами;

```
kubectl get po - l creation_methoh=manual
kubectl get po -l env
kubectl get go -l '!env'
```

#### Резюме
- метки и селекторы меток следует использовать для организации моудлей и упрощения выполнения операций с несколькими модулями одновременно;
- метки и селекторы узлов можно использовать, чтобы назначать модули только тем узлам, котроые имеют определенные функциональные особенности;
- аннотации позволяют приклеклять более крупные блобы данных к модулям, и это делается либо пользователями, либо инструментами и библиотеками;
- пространства имен можно использовать для того, чтобы разные группы специалистов могли использовать тот же кластер, как если бы они использовали разные кластеры K8s;

## Глава 4. Контроллер репликации и другие контроллеры: развертывание управляемых модулей

livenessProbe - проверка текущей работоспособности контейнера
readynessProbe - проверка готовности

Они и используются в разных целях. Проверки могут быть с помощью одного из трех механизмов:
- проверка HTTP GET выполняется HTTP GET на IP-адрес, порт и путь контейнера, которые вы укажите, если ответ не HTTP 2xx и 3xx, контейнер будет перезапущен.
- проверка сокета TCP пытается открыть TCP-подключение к указнному порту контейнера. Если есть подключение, то успех. Иначе контейнер перезапускается.
- проверка Exec выполняет произвольную команду внутри контейнера, exit code 0 == успех, иначе считаются несработавшими

```
kubectl logs mypod --previous
```

Exit code 137. Процесс был заверешнен по внешнему сигналу. Число 137 - это сумма двух чисел: 128 + x, где x - номер сигнала, отправленнного процессу, который вызывал его завершение.
Здесь x == 9, число сигнала SIGKILL, т.е. процесс был убит принудительно.

Обязательно проверяйте только внутренние части приложения и ничего из того, что зависит от внешнего фактора. Например, livenessProbe не должна завершаться с ошибкой, когда
сервер не может подключиться к внутренней базе данных. Если основная причина находится в самой базе данных, перезапуск конейтера веб-сервеа проблему не устранит.

LivenessProbe не должны использоваться слишком много вычислительных ресурсов и не должны занимать слишком много времени.

ReplicationController - это ресурс K8s, который обеспечивает поддержание постоянной работы его модулей.

Цикл контроллера репликации:
1. Начало
2. Найти модули, совпадающие с селектором меток
3. Сравнить совпавшее количество модулей с требуемым, если мало, то п.4, если больше, то п.5, если в самый раз то п.6.
4. Создать доп модулей из текущего шаблона, далее п.6 
5. Удалить лишие модули, далее п.6
6. Вернуться в п.2

Части ReplicationController:
- селектор меток, определеюящие, какие модули находятся в области действия контроллера репликации;
- количество реплик, указывающее на требуемое количество модулей, которые должны быть запущены;
- шаблон модуля, используемый при создании новых реплик модуля;

ReplicationController deprecated. Теперь ReplicaSet.

ReplicaSet имеет более выразительные селекторы модуля, через matchExpressions:
- In
- NotIn
- Exists
- DoesNotExists

DaemonSet - запускают только одну реплику модуля на каждом узле, в том время как ReplicaSet разбрасывают их по всему кластеру случайным образом. Но можно повлиять на это - 
выполнять только на подмножестве всех узлов. Это делается путем задания свойства nodeSelector. Модули DaemonSet деплоятся в обход планировщикова - на них не распространяется unschedulable.

Job позволяет запускать модуль, контейнер которог он не перезапускатеся, когда процесс, запущенный внутри, заканчивается успешно. После этого модуль считается завершенным.
Модули, управляемые opb, переназначиются до тех пор, пока задания не завершатся успешно.

посмотреть завершившиеся тоже:
```
$ kubectl get po -a
```
```
$ kubectl get job
```
activeDeadlineSeconds - органичить время
backoffLimit - количество ретраев

CronJob - в назначенное время создает ресурс Job.

Может случиться так, что задание или модуль создается и выполнятеся относительно поздно. У вас может быть жесткое требование, чтобы задание не отставалло от расписания.
Для этого можно указать startingDeadlineSeconds в секции spec ресурса CronJob. Есои оно не запустится в течение startingDeadlineSeconds, то будет показано Failed.

В обычных условиях CronJob всегда создает всего одно задание для каждого выполнения, но может случиться так, чтоб два задания создаются одновременно или вообще не создаются.
Для борьбы с первой проблемой - задания должны быть идемпотентными. Что касается второго - убедитесь, что следующий запуск задания выполняет любую работу, которая должна была быть выполнена предыдущим (пропущенным) запуском.

#### Резюме
- если задать livenessProbe, то k8s будет перезапускать контейнер, как только он будет не здоров;
- модули не должны создавать напрямую (неотказоустойчиво как минимум);
- контролллеры репликации всегда поддерживаю требуемое количество реплик модуля;
- для горизонатального масштабирования модулей в rc достаточно указать ему желаемое количество реплик модуля;
- rc не владеют модулями, и модули при необходимости могут перемещаться между ними;
- rc создает новые модули из шаблона модуя. Измнение шаблона не влияет на существующие модули;
- rc deprecated. Теперь ReplicaSet (rs) и Deployments
- rc и rs назначают модули случайным узлам кластера, в то время как наборы демонов (DaemonSet) гарантируют, что каждый узел выполняет один экземпляр модуля, определнного в наборе демонов;
- модули, выполняющие пакетную задачу, должны создаваться посредством ресурса Job системы k8s, а не напрямую, или посредством контролера репликации, или анаологичным объектом;
- задания, которые должны выполняться, в будущем, могуть быть созданы с помощью ресурсов CronJob;

## Глава 5. Службы: обеспечение клиентов возможностью обнаруживать модули и обмениваться с ними информацией

k8s service - это единый endpoint для группы группы pod-ов.

```
$ kubectl get svc
```

```
aviVersion:v1
kind: Service
spec:
  sessionAffinity: ClientIP
```
Это заставляет прокси перенаправлять все запросы от одного IP, на тот же модуль.

kube-system namespace:
- kube-dns

fqdn: backend-database.default.svc.cluster.local, где:
- backend-database - имя службы
- default - namespace
- svc.cluster.local - настраиваемый доменный суффикс кластера, используемый во всех именах локальных служб кластера

клас терный IP службы является виртуальным IP-адресом и имеет смысл только в сочетании с портом службы.

```
$ kubectl get endpoints kubia
```

### Предоставление внешний клиентам доступа к службам

Как сделать службу доступной извне:
- присвоить типа службы значение NodePort - для службы NO
- присвоить типу слдубы значение LoadBalancer, расширение типа NodePort
- создать ресурс Ingress, радикально отличающийся механизм предоставления доступа к несколькоим службам через единый IP-адрес - он работает на уровне HTTP (сетевой уровен 7), и может предложить больше возможностей, чем службы уровня 4.

### Headless службы
Если указать `clusterIP: none`, то клиент на DNS запрос буду получать IP-адреса всех подов, входящих в состав службы.

### Устранение неполадок в службах
* прежде всего убедитесь, что вы подключаетесь к кластерному IP-адресу службы изнутри кластера, а не извне
* не тратьте время на пинг IP-адреса службы, чтобы выяснить, доступна ли служба (напомню, что кластерный IP-адрес слжубы это виртуальный IP-адрес, и пинги  не него не работают
* если вы опредилили проверку готовности, убедитесь, что она выполняется успешно; в противном случае модуль не будет частью службы
* для того чтобы убедтиься, чтоб модуль является частью службы, проинспектируйте соответствующий объект Endpoint с помощью команды `kubectl get endpoints`
* если вы пытаетесь получить доступ к службе через ее полное доменное имя или его часть (например, `myservbice.mynamespace.svc.cluster.local` или `myservice.mynamespace`) и это работает, посмотрите, сможете ли вы обратиться к нему, используя ее кластерный IP-адрес вместо полного доменного имени;
* проверьте, подключаетесь ли вы к предоставляемомум службой порту, а не к целевому порту модуля
* попробуйте подлкючится к IP-адресу модуля напрямую, чтобы подтвердить, что вам модуль принимает подключения на правильном порту
* если вы не можете получить доступ к своему приложению даже через IP-адрес модуля, убедитеьс, что ваше приложение не открыто своими портами только к localhost-адресу

### Резюме
Узнали, как k8s:
* обеспечивает доступ к множеству модулей, котрое отождествляются с некоторым селектором меток под единым, стабильным IP-адресом и портом
* делает службы доступными по-умолчанию изнутри кластера, но позволяет сделать службны доступной за пределами кластера, установив для нее тип NodePort или LoadBalancer
* позволяет модулям обнаруживать службы вместе с их IP-адресами и портами путем поиска в переменных окружения
* позволяет обнаруживать службы, расположенные за пределами кластера, и обмениваться с ними информацией, создавая ресурс Service без указания селектора, путем создания ассоциированного ресурса Endpoints
* предоставляет DNS-псевдоним CNAME для внешних служб с типом ExternalName
* обеспечивает доступ к нескольким службам HTTP через один вход Ingress (потребляя один IP-адрес)
* использует проверку готвоности контейнера модуля для определения, должен модуль быть включен в качестве конечной точки службы или нет
* активирует обнаружение IP-адресов модулей через DNS при создании Headless-службы

## Глава 6. Тома: подлкючение дискового хранилища к контейнерам

Том доступен для всех контейнеров модуле, но он должен быть установлен в каждом контейнере, который должен получать к нему доступ.

Том привязан к жизненному циклу модуля и будет существовать только до тех пор, пока модуль существует, но в зависимости от типа тома файлы тома могут оставаться нетронутыми даже после того, как модуль и том исчезнут, а затем могут быть смонтированы в новый том.

Типы томов:
* emptyDir - для временного хранения данных
* hostPath - монитирование из ФС node в модуль
* gitRepo
* nfs
* gcePersistentDisk,...
* cinder, cephfs, iscsi, flocker, glusterfs, quobute, rbd, flexVolume, vsphererVolume, photonPersisntenceDisk, scaleIO
* configMap, secret, downwardAPI - специальные типы томов, используемые для предоставляения модулю определенных ресурсов K8s и кластерной информации
* pesistentVolumeClaim - способ использовать заранее или динаически резервируетмое постоянно хранилище

Постоянные тома PersistentVolume резервируются администраторами кластера и потребляются модулями посредством заявок PersistentVolumeClaim:
1. Админ кластера устанавливает некий тип сетевого хранилища (NFS-экспор или подобное)
2. Админ затем создает том PersistentVolume (PV), отправляя дескриптор PV в API Kubernetes
3. Пользователь создает заявку PersistentVolumeClaim (PVC)
4. Kubernetes находит PV адекватного размера и режиме доступ и связыват заяввку PVC с томом PV
5. Пользователь создаем модуль с томом, ссылающимся на заявку PVC

Ресурсы PV не принадлежат ни к какому пространству имен. Это ресурсы кластерного уровня, такие как узлы.

Режимы доступов:
* RWO - ReadWriteOnce- только один узел может монтироваться том для чтения и записи
* ROX - ReadWriteMany - несколько узлов могут монтировать том для чтения
* RWX - ReadWriteMany - несколько узлов могут монтировать том как для чтения, так и для записи

RWO, ROX, RWX - относятся к числоу рабочих узлов, которые могут использовать том одновременнно, а не к число модулей!

Политики рекламации PV:
- Retain - после высвобождение заявки PV должен быть сохранен
- Recycle - удаляет содержимое тома и делает его доступным для повторного заявки
- Delete - удаляет базовое хранилище

зависит от типа PV, поддерживаемость политик.

#### Динамическое резервирование томов PV

Можно развернуть поставщика (provisioner) ресурса PV и определить один и более объектов StoragaClass, чтобы позволить пользователям выбирать, какой тип ресурса PV они хотят.
Пользователи могут ссылаться на класс хранилища StorageClass в своих завязках PVC, и поставщим будет принмать это во внимание при резрервировании постоянног хранилища.

1. Админ кластера определяет поставщика PV (если он уже не развернут)
2. Админ создает один или несколько ресурсов StorageClass (классов хранилищ) и помечает один как используемый по умолчанию (он может уже существовать)
3. Пользователь создает заявку PVC ссылаясь на один из классов хранилищ (или по умолчанию ни на какой)
4. k8s отыскивает ресурс класс хранилища и поставщика, на который дана в нем ссылка, и просит поставщика зарезервировать новый ресурс PV, основываясь на запрошенном режиме доступа и размере хранилища, запрошенных в заявке PVC, и параметрах в классе StorageClass
5. Поставщик резервирует фактическое хранилище, создает ресурс PV и свзяывает его с заявкой PVC
6. Пользователь создает модуль с томом, ссылающимся на заявку PVC по имени


## Глава 7.Слвари конфигурации (ConfigMap) и секреты (Secret): настройка приложений
Можно настроить приложения с помощью:
- передача в контейнеры аргументов командой строки
- настройки своих собственных переменных среды для каждого контейнера
- монтирования конфигурационных файлов в контейнеры через специальный тип тома

ConfigMap - ассоциативный массив (key-value), где value варьируется от коротких литералов до полных файлов конфигурации

#### Обновление конфигурации приложения без перезапуска приложения

При использовании аргументов cmd line и env vars невозможно их обновить во время выполнения процесса. При использовании тома можно обновлять конфигурациию без необходимости повторного создания модуля или даже перезапуска контейнера.

При обновлении ConfigMap, ссылающиеся на него файлы во всех томах будут обновлены. Далее объязанность процесса обнаружить изменения и перезагрузить их.

Файлы обновляются атомарно из-за использования symlink.

Файлы, смонтированнные в существующие каталоги, не обновлюятся.

#### Secrets

можно:
- передавать записи секрета в контейнер в качестве переменных среды
- предоставлять доступ к записаям секрета в виде файлов в томе

За каждым модуем закреплен том secret. Этот том ссылается на секрет `default-token`. `kubectl describe pod` показывает, где смонтирован том secret.
```
Mounts:
  /var/run/secrets/kubernetes.io/serviceaccount from default-token-cfee9
```
Максимальный размер секрета ограничен 1Мб.

## Глава 8. Доступ к метаданным модуля и другим ресурсам из приложений

downward API дает возможность передавать в контейнеры:
- имя модуля
- IP-адрес модуля
- пространство имен, к которому принадлежит модуль
- имя узла, но котором модуль работает
- имя учетной записи службы, под который модуль работае
- запросы ЦП и памяти для каждого контейнера
- ограничения ЦП и памяти для каждого контейнера
- метки модуля
- аннотации модуля

Метки и аннотации могут быть изменены во время работы модуля. Если доступ к метками и аннотациям модуля бы предоставлен через переменные среды, то нети никакого способа обеспечить доступ к новым значениям после их изменения.

## Взаимодействие с сервером API изнутри модуля
- найти расположение сервера API (`https://kubernetes`)
- убедиться, что вы обмениваетесь с сервером API, а не с тем, что пытается им притворяться
- аутентифицироваться на сервере; в противногом случае он не позволит вам ничего увидеть или сделать

Для упрощения можно использовать:
- контейнер-посредник proxy
- клиентские библиотеки (`https://github.com/kubernetes/client-go`)

## Глава 9. Развертывания: декларативное обновление приложений

```
kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2 --v 6
```

Почему `kubectl rolling-update` устарело:
- меняет логически метки подов
- императивное

Ресурс Deployment нужен чтобы координировать разные ReplicaSet для декларитивного обновления приложений.

Чтобы обновить нужно изменить шаблон модуля, определенный в ресурсе развертывания, и k8s предпримет все необходимые шаги, чтобы привести фактическое состояние к тому, что определено в ресурсе.

Стратегии развертывания:
- Rolling Update 
- Recreate

```kubectl rollout undo deployment <name>```

```kubectl rollout history deployment kubia```

#### Управленение скоростью выкладки

```
kubectl rollout status
```
- maxSurge
- maxUnavailable

Свойство minReadySeconds - предотвращение развертывания неисправных версий


## Глава 10. Ресурсы StatefulSet: развертывание реплицируемых приложений с внутренним состоянием

Все модули из одного набора ReplicaSet всегда используют ту же заявку PVC и PV. Нельзя использовать один набор ReplicaSet для управления распределенным хранилищем данных,
где каждый экземпляр нуждается в отдельном хранилище.

Запуск множества реплик с отдельным хранилищем для каждой:
- создание модулей вручную
- использование одного набора реплик в расчете на экземпляр модуля
- использование множества каталогов в одном томе

Для обеспечения стабильной долговременной идентификации для каждого модуля, можно организовать для каждого отдельного члека выделенную службу K8s. Поскольку IP-адреса служб стабильны,
вы можете указать на каждого участника посредством его IP-адреса службы (а не IP-адрес модуля) в конфигурации.

Объединение этих двух методов не только уродливое, но оно по-прежнему не решает всех вопросов. Отдельные модули не могут знать, через какую службу обеспечивается к ним доступ, поэтому не могут саморегистрироваться в других модулях.

Выход, StatefulSet - набор модулей с внутренним состоянием. 

Когда модуль из StatefulSet умирает, то новый восстановленный экземпляр должен получить то же самое имя, сетевую идентичность и состояние, что и заменяемый. Модули StatefulSet не являются копиями друг друга,
имеют предсказуемую идентичность (порядковые номера), вместо того, чтобы каждый новый экземпляр получал случайную. Headless service.

```
a-0.foo.default.svc.cluster.local
```

При масштабировании вниз набора StatefulSet первым всегда удаляется модуль с наибольшим порядковым номером.

Увеличение масштаба набора StatefulSet по одному модулю создает два или более объектов API ( модуль и одну или несколько PVC). При уменьшении масштаба удаляется только модуль, оставляя заявки в покое.
Требуется удалять заявки PVC вручную.

Объекты StatefulSet создаются и запускаются последовательно.

#### Обнаружение соседей  в наборе StatefulSet
- через Kubernetes API (нежелательно, не платформа-независимый способ)
- DNS, SRV запись

```
dig SRV kubia.default.svc.cluster.local
```

#### Как наборы StatefulSet справляются с аварийныеми сбоями узлов

Поскольку набор StatefulSet гарантирует, что никогда не будет двух модулей, работающих с одной и той же идентичностью и хранилищем, 
при аварийном сбое узла набор StatefulSet не может и не должен создавать сменный модуль до тех пор, пока не будет точно известно, что модуль больше не работает.

Он может узнать это только когда, когда об этом соообщит администратор кластера. Для этого админ должен удалить модуль или удалить весь узел.

При аварийном завершении Node (Not Ready) статус модуля становится Unknown. Если Kubelet видит ведущего узла (н-р, при сбое сети), модуль будет продолжать работать,
т.к. Kubelet завершает работу модуля, если видит что модуль помечен на удаление.

```
kubectl delete po kubia-0 --force --grace-period 0
```
!!! Не удаляйте модули с внутренним состоянием принудительно, если вы знаете, что узел больше не работает или недоступен (и останется в таком состоянии навсегда)

### Глава 11. Внутреннее устройство Kubernetes

Компоненты плоскости управления:
- etcd
- API server
- scheduler
- controller manager

Компоненты, работающие на рабочих узлах:
- агенты Kubelet
- служебный прокси Kubernetes (kube-proxy)
- среда выполнения контейнеров (Docker, rkt или др.)

Дополнительные компоненты:
- DNS-сервер K8s
- Dashboard
- Ingress Controller
- Heapster
- сетевой плагин контейнерого сетевого интерфейса

Шедулер, принятый по-умолчанию алгоритм назначения:
- фильтрация списка всех узлов для получения списка приемливых узлов, на которые можно назначить модуль
- приоритезация приемллимых узлов и выбор самого лучшего. Используется циклический перебор среди лучших, чтобы гарантировать, что модули развернуты по всем из них равномерно

Механизм watch не гарантирует, что контроллер не пропустит событие, они так же периодически выполняют операцию запроса списка, чтобы убедиться, что они ничего не пропустили.

#### Запуск высокодоступных кластеров

Приложения:
- запуск нескольких экземпляров для снижения вероятности простоя
- применение выборов лидера для немасштабируемых горизонтальных приложений

Для leader-election в control-plane используется ресурс endpoint - `kube-scheduler`:
```
$ kubectl get ep kube-scheduler -n kube-system -o yaml
...
    control-plane.alpha.kubernetes.io.leader: ...
...
```